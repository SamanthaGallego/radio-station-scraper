{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMUa4LitsXo2iEcNN5gXh/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamanthaGallego/radio-station-scraper/blob/main/radio_station_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"SamanthaGallego\"\n",
        "!git config --global user.email \"gallegosamantha611@gmail.com\"\n"
      ],
      "metadata": {
        "id": "wmR9fYx9Lndn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SamanthaGallego/radio-station-scraper.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMPBJMlEL2dM",
        "outputId": "eef0bc1c-f4e3-4733-f3af-e954d1d51ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'radio-station-scraper'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd radio-station-scraper\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGX8DmEUL8XT",
        "outputId": "d5e9ec0f-7ca8-4a23-8266-98cbf1dd53a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/radio-station-scraper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de las librerías necesarias\n",
        "!pip install duckduckgo-search requests-html lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqrxnkeKAjwi",
        "outputId": "6cb360a9-0a3e-4cb8-b9e1-3d755cae556f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-7.3.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.1.8)\n",
            "Collecting primp>=0.11.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.12.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.32.3)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-2.0.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting bs4 (from requests-html)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (2024.12.14)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.6.1)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4->requests-html) (4.12.3)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->requests-html) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->requests-html) (3.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4->requests-html) (2.6)\n",
            "Downloading duckduckgo_search-7.3.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading primp-0.12.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading fake_useragent-2.0.3-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, primp, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, duckduckgo-search, bs4, requests-html\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 0.3.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.2.0 duckduckgo-search-7.3.1 fake-useragent-2.0.3 lxml_html_clean-0.4.1 parse-1.20.2 primp-0.12.0 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import DDGS\n",
        "from requests_html import HTMLSession\n",
        "import re\n",
        "import csv\n",
        "import openpyxl\n"
      ],
      "metadata": {
        "id": "UXsDVhu3Dp_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Función para buscar URLs de emisoras en Long Island utilizando DuckDuckGo\n",
        "def buscar_urls_emisoras(query, max_results=30):\n",
        "    urls = []\n",
        "    with DDGS() as ddgs:\n",
        "        for result in ddgs.text(f\"{query} Long Island radio stations site:.com\", region='us-en'):\n",
        "            if len(urls) >= max_results:\n",
        "                break\n",
        "            urls.append(result['href'])\n",
        "    return urls\n",
        "\n",
        "# Función para buscar correos electrónicos en una URL\n",
        "def extraer_correos(url):\n",
        "    session = HTMLSession()\n",
        "    try:\n",
        "        response = session.get(url)\n",
        "        correos = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", response.text)\n",
        "        return list(set(correos))  # Eliminar duplicados\n",
        "    except Exception as e:\n",
        "        print(f\"Error al acceder a {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Función para extraer redes sociales (Facebook, Instagram) de una URL\n",
        "def extraer_redes_sociales(url):\n",
        "    session = HTMLSession()\n",
        "    try:\n",
        "        response = session.get(url)\n",
        "        facebook = re.findall(r'(https?://(?:www\\.)?facebook\\.com/[a-zA-Z0-9._%+-]+)', response.text)\n",
        "        instagram = re.findall(r'(https?://(?:www\\.)?instagram\\.com/[a-zA-Z0-9._%+-]+)', response.text)\n",
        "        return facebook, instagram\n",
        "    except Exception as e:\n",
        "        print(f\"Error al acceder a {url}: {e}\")\n",
        "        return [], []\n",
        "\n",
        "# Guardar los datos (correos, redes sociales y URL) en un archivo Excel\n",
        "def guardar_datos_excel(datos, archivo=\"emisoras_datos.xlsx\"):\n",
        "    # Crear el archivo Excel\n",
        "    wb = openpyxl.Workbook()\n",
        "    sheet = wb.active\n",
        "    sheet.title = \"Emisoras\"\n",
        "\n",
        "    # Escribir encabezados\n",
        "    sheet.append([\"URL Emisora\", \"Correo\", \"Facebook\", \"Instagram\"])\n",
        "\n",
        "    # Escribir los datos de las emisoras\n",
        "    for fila in datos:\n",
        "        sheet.append(fila)\n",
        "\n",
        "    # Guardar el archivo\n",
        "    wb.save(archivo)\n",
        "    print(f\"Datos guardados en {archivo}\")"
      ],
      "metadata": {
        "id": "bnRqg-SCAheK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecución del script\n",
        "query = \"contact email\"\n",
        "print(\"Buscando URLs de emisoras en Long Island...\")\n",
        "urls = buscar_urls_emisoras(query, max_results=100)  # Puedes aumentar el límite\n",
        "\n",
        "print(\"\\nURLs encontradas:\")\n",
        "for url in urls:\n",
        "    print(url)\n",
        "\n",
        "# Crear una lista de todos los datos (URLs, correos, redes sociales)\n",
        "todos_los_datos = []\n",
        "\n",
        "for url in urls:\n",
        "    correos = extraer_correos(url)\n",
        "    facebook, instagram = extraer_redes_sociales(url)\n",
        "    for correo in correos:\n",
        "        todos_los_datos.append([url, correo, ', '.join(facebook), ', '.join(instagram)])\n",
        "\n",
        "# Guardar los datos en un archivo Excel\n",
        "if todos_los_datos:\n",
        "    guardar_datos_excel(todos_los_datos)\n",
        "else:\n",
        "    print(\"No se encontraron correos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhPhelo3A40l",
        "outputId": "1c393245-c325-400a-e4a7-f4b7bb172ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buscando URLs de emisoras en Long Island...\n",
            "\n",
            "URLs encontradas:\n",
            "https://linewsradio.com/contact/\n",
            "https://www.kjoy.com/contact-us/\n",
            "https://www.longisland.com/radio-stations/\n",
            "https://longisland.news12.com/pages/long-island-contact\n",
            "https://licountry.com/contact/station-info\n",
            "https://wehm.com/contact/station-info\n",
            "https://party105.com/contact/station-info\n",
            "https://www.liblues.com/contact\n",
            "https://www.iradiousa.com/about\n",
            "https://connoisseurmedia.com/\n",
            "Datos guardados en emisoras_datos.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx4r-qB-N9z3",
        "outputId": "6bfe35a9-45e0-4f6a-c647-071a36317b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wICmc_xPOGAN",
        "outputId": "131b7387-1f7e-4527-c433-f07c8e64729e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xPr7vFJ3P_k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content -name \"radio_station_scraper.ipynb\"\n"
      ],
      "metadata": {
        "id": "V0sFYEJZOTta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/radio_station_scraper.ipynb /content/radio-station-scraper/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2sa2YqOM90C",
        "outputId": "90637711-d1be-4fd6-ce7a-f298db759eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/radio_station_scraper.ipynb': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yy5kDNsoNF1h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}